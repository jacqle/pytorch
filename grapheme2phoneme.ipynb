{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G2P\n",
    "\n",
    "### Grapheme to phoneme conversion with the cmudict dataset\n",
    "\n",
    "Inspired by Ben Trevett's excellent [tutorial](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) on seq2seq learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91IXF8ivk-RG",
    "outputId": "9a065a68-c066-4670-a7bc-9c1337c86590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in /usr/local/lib/python3.6/dist-packages (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance # to calculate phoneme error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "an_jWBZQk-RP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re \n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import textdistance\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdUx6JAWk-RR"
   },
   "source": [
    "## 1. Preprocess the data + Custom dataloader and collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnNVkfOHk-RR"
   },
   "source": [
    "Set seed for determinic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_UqSK8r_k-RS"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU6DSylFk-RS"
   },
   "source": [
    "We create a custom Dataset class to pass our datasplits (see below) to pytorch's DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "00nB6gAIk-RT"
   },
   "outputs": [],
   "source": [
    "class Subset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Subset class to pass to DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        data (list): numericalized data pairs\n",
    "        source_field (Field): converts a source example back into a word\n",
    "        target_field (Field): converts a target example back into phonemes\n",
    "    \"\"\"\n",
    "    def __init__(self, data, source_field, target_field):\n",
    "        self.data = data\n",
    "        self.source_field = source_field\n",
    "        self.target_field = target_field\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def to_string(self, index):\n",
    "        \"\"\"Converts an example pair back into string.\"\"\"\n",
    "        \n",
    "        source, target = self.data[index]\n",
    "        decoded_source = self.source_field.tensor_to_string(source, datatype='chars')\n",
    "        decoded_target = self.target_field.tensor_to_string(target, datatype='phonemes')\n",
    "        return decoded_source, decoded_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmd1jZ4-k-RT"
   },
   "source": [
    "This Field class handles source and target data to convert strings to tensors according to a vocabulary mapping (built from the training data only). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E37djnxZk-RT"
   },
   "outputs": [],
   "source": [
    "class Field:\n",
    "    \"\"\"\n",
    "    A Field class to create a vocabulary and numericalize data.\n",
    "    Args:\n",
    "        string2int (dict): mapping from string to indices\n",
    "        data (list): example strings (words or phonemes)\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.string2int = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.string2int)\n",
    "    \n",
    "    def build_vocab(self, data):\n",
    "        idx = 4\n",
    "        for string in data:\n",
    "            for char in string:\n",
    "                if not char in self.string2int:\n",
    "                    self.string2int[char] = idx\n",
    "                    idx += 1               \n",
    "        self.int2string = {v: k for k, v in self.string2int.items()}\n",
    "        \n",
    "    def string_to_tensor(self, string):\n",
    "        \"\"\"Converts a string according to the vocabulary and adds special tokens.\"\"\"\n",
    "        \n",
    "        int_string = [self.string2int[\"<SOS>\"]]\n",
    "        int_string.extend([self.string2int[char] if char in self.string2int \n",
    "                             else self.string2int[\"<UNK>\"] for char in string])\n",
    "        int_string.append(self.string2int[\"<EOS>\"])\n",
    "        tensor = torch.LongTensor(int_string)\n",
    "        return tensor   \n",
    "    \n",
    "    def tensor_to_string(self, tensor, datatype):\n",
    "        \"\"\"Translates a tensor mapping back into text.\"\"\"\n",
    "        \n",
    "        if datatype == 'chars':\n",
    "            return \"\".join(self.int2string[int(i.item())] for i in tensor if not i in [0, 1, 2])\n",
    "        else:\n",
    "            return \" \".join(self.int2string[int(i.item())] for i in tensor if not i in [0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlFUdXM9k-RU"
   },
   "source": [
    "The GraphemesDataset class:\n",
    " - loads the data, preprocesses it and shuffles it\n",
    " - partitions the numericalized data into a train/dev/test split (as our custom Subset class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hfbVhC2kk-RV"
   },
   "outputs": [],
   "source": [
    "class GraphemesDataset:\n",
    "    \"\"\"\n",
    "    A class to load the data and create numericalized data splits.\n",
    "    Args:\n",
    "        dict_file (str): path to data file\n",
    "        split (list): proportions to split the data into train/dev/test\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_file, split):\n",
    "        self.dict_file = dict_file\n",
    "        self.split = split\n",
    "        self.data = self._get_data(dict_file)\n",
    "        random.shuffle(self.data)        \n",
    "        self.source_field = Field([ex[0] for ex in self.data])\n",
    "        self.target_field = Field([ex[1] for ex in self.data])\n",
    "        self.train_data, self.valid_data, self.test_data = self._split_dataset()\n",
    "        self._build_vocab(self.train_data)\n",
    "                    \n",
    "    def _get_data(self, dict_file):\n",
    "        \"\"\"Retrieves source and target data for a file and preprocesses it.\"\"\"\n",
    "        \n",
    "        data_pairs = []\n",
    "        with open(dict_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                word = line.strip().split()[0] \n",
    "                if not re.search('(\\d)', word): # ignore repeated samples\n",
    "                    line = line.strip().split(' # ')[0] # remove content with #\n",
    "                    ex_source, ex_target = line.split(' ', 1) \n",
    "                    data_pairs.append((ex_source, ex_target.split()))\n",
    "        return data_pairs\n",
    "    \n",
    "    def _split_dataset(self):\n",
    "        \"\"\"Splits the data according to the defined proportions.\"\"\"\n",
    "        \n",
    "        train, valid, test = self.split\n",
    "        train_index = int(len(self.data) * train)\n",
    "        valid_index = train_index + int(len(self.data) * valid)\n",
    "        train_data = []\n",
    "        valid_data = []\n",
    "        test_data = []\n",
    "        for idx, (source_ex, target_ex) in enumerate(self.data):\n",
    "            if idx < train_index:\n",
    "                train_data.append((source_ex, target_ex))\n",
    "            elif train_index <= idx < valid_index:\n",
    "                valid_data.append((source_ex, target_ex))\n",
    "            else:\n",
    "                test_data.append((source_ex, target_ex))\n",
    "        return train_data, valid_data, test_data\n",
    "    \n",
    "    def _build_vocab(self, train_data):\n",
    "        \"\"\"Builds vocabulary mappings for source and target.\"\"\"\n",
    "        \n",
    "        source = [ex[0] for ex in train_data]\n",
    "        target = [ex[1] for ex in train_data]\n",
    "        self.source_field.build_vocab(source)\n",
    "        self.target_field.build_vocab(target)\n",
    "        \n",
    "    def _to_tensor(self, pair):\n",
    "        \"\"\"Converts a string to a tensor according to the vocabulary.\"\"\"\n",
    "        \n",
    "        source, target = pair\n",
    "        encoded_source = self.source_field.string_to_tensor(source)\n",
    "        encoded_target = self.target_field.string_to_tensor(target)\n",
    "        return encoded_source, encoded_target \n",
    "\n",
    "    def get_transformed_data_splits(self):\n",
    "        \"\"\"Gets numericalized Subsets from the data splits for DataLoader.\"\"\"\n",
    "        \n",
    "        train_data = Subset([self._to_tensor(ex) for ex in self.train_data], \n",
    "                               self.source_field, self.target_field)\n",
    "        valid_data = Subset([self._to_tensor(ex) for ex in self.valid_data],\n",
    "                               self.source_field, self.target_field)\n",
    "        test_data = Subset([self._to_tensor(ex) for ex in self.test_data],\n",
    "                              self.source_field, self.target_field)\n",
    "        return train_data, valid_data, test_data       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqynXCx_k-RW"
   },
   "source": [
    "We use a custom collate function for the Dataloader which pads sequences according to the longest sequence in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Kvo-DyOIk-RW"
   },
   "outputs": [],
   "source": [
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    A custom Collate class to pass as collate_fn to DataLoader.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=0):\n",
    "        self.dim = dim\n",
    "\n",
    "    def _pad_collate(self, batch):\n",
    "        # find longest sequence\n",
    "        max_len_source = max(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        max_len_target = max(map(lambda x: x[1].shape[self.dim], batch))\n",
    "        max_len = max((max_len_source, max_len_target))\n",
    "        # pad according to max_len\n",
    "        source = [self.pad_tensor(source, pad=max_len) for source, target in batch] \n",
    "        target = [self.pad_tensor(target, pad=max_len) for source, target in batch]\n",
    "        # stack all\n",
    "        src = torch.stack(source, dim=1)\n",
    "        trg = torch.stack(target, dim=1)\n",
    "        return src, trg\n",
    "\n",
    "    def pad_tensor(self, vec, pad):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vec - tensor to pad\n",
    "            pad - the size to pad to\n",
    "        Return:\n",
    "            a new tensor padded to 'pad' in dimension 'dim'\n",
    "        \"\"\"\n",
    "        pad_size = list(vec.shape)\n",
    "        pad_size[self.dim] = pad - vec.size(self.dim)\n",
    "        return torch.cat([vec, torch.zeros(*pad_size).long()], dim=self.dim)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self._pad_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jLUitOl1k-RX"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "dict_file = \"cmudict.dict\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = GraphemesDataset(dict_file=dict_file, split=[0.85, 0.05, 0.1])\n",
    "train_data, valid_data, test_data = dataset.get_transformed_data_splits()\n",
    "\n",
    "train_iterator = DataLoader(train_data, shuffle=True, num_workers=NUM_WORKERS, drop_last=True,  \n",
    "                            batch_size=BATCH_SIZE, collate_fn=PadCollate(), pin_memory=True)\n",
    "valid_iterator = DataLoader(valid_data, batch_size=BATCH_SIZE, collate_fn=PadCollate(), drop_last=True)\n",
    "test_iterator = DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=PadCollate(), drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TLLU1eo-k-RY"
   },
   "outputs": [],
   "source": [
    "source_vocab = dataset.source_field.int2string\n",
    "target_vocab = dataset.target_field.int2string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8-hi5WzQk-RY",
    "outputId": "020f4ae6-1632-4a3f-af76-45fa8eef7b00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 107138\n",
      "Number of validation examples: 6302\n",
      "Number of testing examples: 12605\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBxtOVpxk-RZ"
   },
   "source": [
    "Subset method to check an example at a given index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWGHia8qk-RZ",
    "outputId": "d3bb57e4-cf7f-4fa3-f8ff-1ace9e27f249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('radiography', 'R EY2 D IY0 AA1 G R AH0 F IY0')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.to_string(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGE1MA9zk-Ra",
    "outputId": "9fc31f39-a026-4a53-daf7-5511d87a74bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (characters) vocabulary: 33\n",
      "Unique tokens in target (phonemes) vocabulary: 73\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (characters) vocabulary: {len(dataset.source_field)}\")\n",
    "print(f\"Unique tokens in target (phonemes) vocabulary: {len(dataset.target_field)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_Fyib9vk-Ra"
   },
   "source": [
    "## 3. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "twVXvvB2k-Rb"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uJZ9khcwk-Rb"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nGZol0Vmk-Rb"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vouOHhck-Rc"
   },
   "source": [
    "## 4. Hyperparameters and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VdTIck3Yk-Rd"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(dataset.source_field)\n",
    "OUTPUT_DIM = len(dataset.target_field)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIPwDPcpk-Rd",
    "outputId": "58642a5b-0f61-4035-858a-b440cef1bbd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(33, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(73, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=73, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qFRLvRnk-Rd",
    "outputId": "b1f346bc-91fa-44bd-c93b-4d59c0de3d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,421,001 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gYwSbu5jk-Re"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EFMzS7__k-Re"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = dataset.source_field.string2int[\"<PAD>\"]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj1vew4vk-Re"
   },
   "source": [
    "## 5. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "V-P-v_lkk-Re"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, trg = batch\n",
    "\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src.long(), trg.long())\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4WvTLizTine"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "We use the following common evaluation metrics for G2P task, taken from the paper \"Transformer based Grapheme-to-Phoneme Conversion\" by Yolchuyeva, Németh, Gyires-Tóth, Interspeech 2019:\n",
    "\n",
    "- Phoneme Error Rate (PER): Levenshtein distance between the predicted phoneme sequence and the target sequence, divided by  the number of phonemes in the target pronunciation; this is done with all target-prediction pairs, and an average is then computed.\n",
    "- Word Error Rate (WER): percentage of words in which the  predicted phoneme sequence does not exactly match the target pronunciation; the number of word errors is divided by the number of test samples.\n",
    "\n",
    "We use a flag `test` to differenciate between evaluation and test mode. In test mode, each test example is fed into a softmax to have a probability distribution over the target vocabulary. We retrieve the most likely candidate using `argmax`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2W-X-cOSoJ5s"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, test):\n",
    "    \n",
    "    model.eval()\n",
    "    softmax = torch.nn.Softmax(dim=0)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    wer_count = 0 # count to calculate word error rate\n",
    "    levenshtein_scores = [] # list to calculate phoneme error rate\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            \n",
    "        for batch in iterator:\n",
    "\n",
    "            src, trg = batch\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            # reshape data for evaluation of each test example\n",
    "            trg_T = torch.transpose(trg, 0, 1) \n",
    "            output_T = torch.transpose(output, 0, 1)\n",
    "\n",
    "            for target_seq, output_seq in zip(trg_T, output_T):\n",
    "                \n",
    "                #target_seq = [trg len]\n",
    "                #output_seq = [trg len, output dim]\n",
    "\n",
    "                # flags to signal when both target and output sequences are over\n",
    "                # i.e. ignore padding\n",
    "                trg_stopped = False \n",
    "                output_stopped = False\n",
    "\n",
    "                decoded_target_seq  = []\n",
    "                decoded_output_seq = []                \n",
    "\n",
    "                for y, pred in zip(target_seq[1:], output_seq[1:]): # ignore <SOS>\n",
    "\n",
    "                    #y = single value\n",
    "                    #pred = [output dim]\n",
    "\n",
    "                    pred = softmax(pred)\n",
    "                    y_hat = torch.argmax(pred)\n",
    "\n",
    "                    if not output_stopped and target_vocab[y_hat.item()] == '<EOS>':\n",
    "                        output_stopped = True\n",
    "                    if not trg_stopped and target_vocab[y.item()] == '<EOS>':\n",
    "                        trg_stopped = True \n",
    "                    if output_stopped and trg_stopped:\n",
    "                        break\n",
    "\n",
    "                    y = target_vocab[y.item()]\n",
    "                    y_hat = target_vocab[y_hat.item()]\n",
    "\n",
    "                    decoded_target_seq.append(y)\n",
    "                    decoded_output_seq.append(y_hat)\n",
    "\n",
    "                predictions.append(' '.join(decoded_target_seq) + '\\t' + \n",
    "                                   ' '.join(decoded_output_seq))\n",
    "\n",
    "                # WER\n",
    "                if decoded_target_seq != decoded_output_seq:\n",
    "                    wer_count += 1\n",
    "\n",
    "                # PER\n",
    "                seq_len = target_seq[1:].tolist().index(2) # 2: <EOS>\n",
    "                levenshtein_scores.append(\n",
    "                    textdistance.levenshtein.distance(\n",
    "                        decoded_target_seq, decoded_output_seq) / seq_len)\n",
    "                \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    if test: # write to file for qualitative analysis\n",
    "\n",
    "        with open('predictions.tsv', 'w') as ostr:\n",
    "            for pred in predictions:\n",
    "                print(pred, file=ostr)\n",
    "\n",
    "        WER = wer_count / len(test_data)    \n",
    "        PER = sum(levenshtein_scores) / len(levenshtein_scores)\n",
    "\n",
    "        return (epoch_loss / len(iterator)), WER, PER\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b3DeoSQvk-Rf"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQyUDqdXk-Rg",
    "outputId": "58a647ec-d3c1-4210-d008-a080e4db0f03",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 7s\n",
      "\tTrain Loss: 2.008 | Train PPL:   7.452\n",
      "\t Val. Loss: 0.995 |  Val. PPL:   2.704\n",
      "Epoch: 02 | Time: 1m 7s\n",
      "\tTrain Loss: 0.644 | Train PPL:   1.904\n",
      "\t Val. Loss: 0.612 |  Val. PPL:   1.844\n",
      "Epoch: 03 | Time: 1m 8s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.515 |  Val. PPL:   1.674\n",
      "Epoch: 04 | Time: 1m 6s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.467 |  Val. PPL:   1.596\n",
      "Epoch: 05 | Time: 1m 7s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.561\n",
      "Epoch: 06 | Time: 1m 7s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 07 | Time: 1m 6s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 08 | Time: 1m 8s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.515\n",
      "Epoch: 09 | Time: 1m 7s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 10 | Time: 1m 6s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.496\n",
      "Epoch: 11 | Time: 1m 8s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 12 | Time: 1m 7s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 13 | Time: 1m 7s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.514\n",
      "Epoch: 14 | Time: 1m 8s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 15 | Time: 1m 8s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 16 | Time: 1m 6s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 17 | Time: 1m 7s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.538\n",
      "Epoch: 18 | Time: 1m 7s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 19 | Time: 1m 7s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.561\n",
      "Epoch: 20 | Time: 1m 7s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion, test=False)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss: # save the best model\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'g2p-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yqhmP35WxUA"
   },
   "source": [
    "Plotting train and validation losses over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "4LdgbO8nNegT",
    "outputId": "eb94fbb7-1ac0-4542-cf83-5e5a8ab58ccf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, 'r', label=\"train\")\n",
    "plt.plot(valid_losses, 'b', label=\"validation\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmEijhZNk-Rg"
   },
   "source": [
    "## 6. Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKUQsjQfk-Rg",
    "outputId": "44f73196-34c6-449f-8637-e3b547fdc59e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('g2p-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRJ-YzAgk-Rh",
    "outputId": "986091cf-c23b-4a10-b7aa-f494dae8bd78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Word Error Rate: 0.374 |\n",
      "| Phoneme Error Rate: 0.109 |\n",
      "| Test Loss: 0.420 | Test PPL:   1.521 |\n"
     ]
    }
   ],
   "source": [
    "test_loss, WER, PER = evaluate(model, test_iterator, criterion, test=True)\n",
    "print(f'| Word Error Rate: {WER:.3f} |')\n",
    "print(f'| Phoneme Error Rate: {PER:.3f} |')\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "grapheme2phoneme.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
